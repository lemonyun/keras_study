{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.1-one-hot-encoding-of-words-or-characters.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvGtMN1Ntk7XI4yUxDmnTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lemonyun/keras_study/blob/main/6_1_one_hot_encoding_of_words_or_characters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 데이터\n",
        "\n",
        "1. 시계열 비교 - 두 문서나 주식 가격이 얼마나 밀접하게 관련이 있는지 추정\n",
        "2. 시퀀스-투-시퀀스 학습 - 영어 문장을 프랑스어로 변환하기\n",
        "3. 감성 분석 - 영화 리뷰의 긍정 부정 분류\n",
        "4. 시계열 예측 - 어떤 지역의 최근 날씨 데이터가 주어졌을 때 향후 날씨 예측하기"
      ],
      "metadata": {
        "id": "IUGCK0a_bLJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# n-gram과 BoW\n",
        "\n",
        "BoW는 n-gram의 집합인데 특정한 순서가 없기 때문에 딥러닝 모델보다 얕은 학습 방법의 언어 처리 모델에 사용되는 경향이 있음\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BxvT5iUQbAuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jsRB-hfFdVua",
        "outputId": "652a99f9-42da-4389-9db1-61a0b6551002"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단어 수준 원-핫 인코딩"
      ],
      "metadata": {
        "id": "GLPuDFGMfV1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 초기 데이터: 각 원소가 샘플입니다\n",
        "# (이 예에서 하나의 샘플이 하나의 문장입니다. 하지만 문서 전체가 될 수도 있습니다)\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# 데이터에 있는 모든 토큰의 인덱스를 구축합니다\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "    # split() 메서드를 사용해 샘플을 토큰으로 나눕니다.\n",
        "    # 실전에서는 구둣점과 특수 문자도 사용합니다.\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            # 단어마다 고유한 인덱스를 할당합니다.\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            # 인덱스 0은 사용하지 않습니다.\n",
        "\n",
        "# 샘플을 벡터로 변환합니다.\n",
        "# 각 샘플에서 max_length 까지 단어만 사용합니다.\n",
        "max_length = 10\n",
        "\n",
        "# 결과를 저장할 배열입니다\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1."
      ],
      "metadata": {
        "id": "EMJdjIvtdXsC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tJAzdzgdZ24",
        "outputId": "eb1288e7-ef4b-4371-84cd-1b8e17c394ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R_ntsbrebwR",
        "outputId": "65e25e2a-c351-4b81-ccbe-21655d55e357"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The': 1,\n",
              " 'ate': 8,\n",
              " 'cat': 2,\n",
              " 'dog': 7,\n",
              " 'homework.': 10,\n",
              " 'mat.': 6,\n",
              " 'my': 9,\n",
              " 'on': 4,\n",
              " 'sat': 3,\n",
              " 'the': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(len(samples), max_length, max(token_index.values()) + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_pioKcAevGI",
        "outputId": "7c961229-29c1-4d39-cfe7-25efea392942"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_index.values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSImxk0MfESa",
        "outputId": "30dde739-6180-4942-8621-c163b72663d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문자 수준 원-핫 인코딩"
      ],
      "metadata": {
        "id": "ZnNN8sx7fOGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "zip() 함수는 여러 개의 순회 가능한(iterable) 객체를 인자로 받고, 각 객체가 담고 있는 원소를 터플의 형태로 차례로 접근할 수 있는 반복자(iterator)를 반환"
      ],
      "metadata": {
        "id": "PRI9iZH3h2K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable  # 출력 가능한 모든 아스키(ASCII) 문자\n",
        "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, character in enumerate(sample[:max_length]):\n",
        "        index = token_index.get(character)\n",
        "        results[i, j, index] = 1."
      ],
      "metadata": {
        "id": "l5pAZbpnfc9Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.printable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0sPuwexhgV2D",
        "outputId": "3147c4ad-9865-4ce8-c0eb-bdacfce72434"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKyt6u9Bga04",
        "outputId": "12707819-fc0a-4de9-e14a-d265ba7eacb0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\t': 96,\n",
              " '\\n': 97,\n",
              " '\\x0b': 99,\n",
              " '\\x0c': 100,\n",
              " '\\r': 98,\n",
              " ' ': 95,\n",
              " '!': 63,\n",
              " '\"': 64,\n",
              " '#': 65,\n",
              " '$': 66,\n",
              " '%': 67,\n",
              " '&': 68,\n",
              " \"'\": 69,\n",
              " '(': 70,\n",
              " ')': 71,\n",
              " '*': 72,\n",
              " '+': 73,\n",
              " ',': 74,\n",
              " '-': 75,\n",
              " '.': 76,\n",
              " '/': 77,\n",
              " '0': 1,\n",
              " '1': 2,\n",
              " '2': 3,\n",
              " '3': 4,\n",
              " '4': 5,\n",
              " '5': 6,\n",
              " '6': 7,\n",
              " '7': 8,\n",
              " '8': 9,\n",
              " '9': 10,\n",
              " ':': 78,\n",
              " ';': 79,\n",
              " '<': 80,\n",
              " '=': 81,\n",
              " '>': 82,\n",
              " '?': 83,\n",
              " '@': 84,\n",
              " 'A': 37,\n",
              " 'B': 38,\n",
              " 'C': 39,\n",
              " 'D': 40,\n",
              " 'E': 41,\n",
              " 'F': 42,\n",
              " 'G': 43,\n",
              " 'H': 44,\n",
              " 'I': 45,\n",
              " 'J': 46,\n",
              " 'K': 47,\n",
              " 'L': 48,\n",
              " 'M': 49,\n",
              " 'N': 50,\n",
              " 'O': 51,\n",
              " 'P': 52,\n",
              " 'Q': 53,\n",
              " 'R': 54,\n",
              " 'S': 55,\n",
              " 'T': 56,\n",
              " 'U': 57,\n",
              " 'V': 58,\n",
              " 'W': 59,\n",
              " 'X': 60,\n",
              " 'Y': 61,\n",
              " 'Z': 62,\n",
              " '[': 85,\n",
              " '\\\\': 86,\n",
              " ']': 87,\n",
              " '^': 88,\n",
              " '_': 89,\n",
              " '`': 90,\n",
              " 'a': 11,\n",
              " 'b': 12,\n",
              " 'c': 13,\n",
              " 'd': 14,\n",
              " 'e': 15,\n",
              " 'f': 16,\n",
              " 'g': 17,\n",
              " 'h': 18,\n",
              " 'i': 19,\n",
              " 'j': 20,\n",
              " 'k': 21,\n",
              " 'l': 22,\n",
              " 'm': 23,\n",
              " 'n': 24,\n",
              " 'o': 25,\n",
              " 'p': 26,\n",
              " 'q': 27,\n",
              " 'r': 28,\n",
              " 's': 29,\n",
              " 't': 30,\n",
              " 'u': 31,\n",
              " 'v': 32,\n",
              " 'w': 33,\n",
              " 'x': 34,\n",
              " 'y': 35,\n",
              " 'z': 36,\n",
              " '{': 91,\n",
              " '|': 92,\n",
              " '}': 93,\n",
              " '~': 94}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV8gxoJ4hpee",
        "outputId": "13b913aa-5807-4cdf-b375-2fbcb5e183a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 50, 101)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwOnZ6VwjA5r",
        "outputId": "6397263f-0d39-463f-c14c-2739120b7c18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스에는 원본 텍스트 데이터를 단어 또는 문자 수준의 원-핫 인코딩으로 변환해주는 유틸리티가 있습니다. 특수 문자를 제거하거나 빈도가 높은 N개의 단어만을 선택(입력 벡터 공간이 너무 커지지 않도록 하기 위한 일반적인 제한 방법입니다)하는 등 여러 가지 중요한 기능들이 있기 때문에 이 유틸리티를 사용하는 것이 좋습니다.\n",
        "\n",
        "케라스를 사용한 단어 수준의 원-핫 인코딩:"
      ],
      "metadata": {
        "id": "a_KTvNnZjFp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# 가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer 객체를 만듭니다.\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "# 단어 인덱스를 구축합니다.\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "# 문자열을 정수 인덱스의 리스트로 변환합니다.\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "# 직접 원-핫 이진 벡터 표현을 얻을 수 있습니다.\n",
        "# 원-핫 인코딩 외에 다른 벡터화 방법들도 제공합니다!\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "# 계산된 단어 인덱스를 구합니다.\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHL3XZiDlGgg",
        "outputId": "4aa48b36-239a-42f5-cdad-3a00f964c443"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNIn50_nlHjK",
        "outputId": "bd4ca197-56a6-4987-ce32-a50b4105d131"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ate': 7,\n",
              " 'cat': 2,\n",
              " 'dog': 6,\n",
              " 'homework': 9,\n",
              " 'mat': 5,\n",
              " 'my': 8,\n",
              " 'on': 4,\n",
              " 'sat': 3,\n",
              " 'the': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7tafo2ylMGt",
        "outputId": "72b90818-7416-4970-bf70-175205cb6e33"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKrFPbAMlVvZ",
        "outputId": "25211260-2125-4ac5-fafc-c7c54ecc5375"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "원-핫 인코딩의 변종 중 하나는 원-핫 해싱 기법입니다. 이 방식은 어휘 사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울 때 사용합니다. 각 단어에 명시적으로 인덱스를 할당하고 이 인덱스를 딕셔너리에 저장하는 대신에 단어를 해싱하여 고정된 크기의 벡터로 변환합니다. 일반적으로 간단한 해싱 함수를 사용합니다. 이 방식의 주요 장점은 명시적인 단어 인덱스가 필요 없기 때문에 메모리를 절약하고 온라인 방식으로 데이터를 인코딩할 수 있습니다(전체 데이터를 확인하지 않고 토큰을 생성할 수 있습니다). 한 가지 단점은 해시 충돌입니다. 두 개의 단어가 같은 해시를 만들면 이를 바라보는 머신 러닝 모델은 단어 사이의 차이를 인식하지 못합니다. 해싱 공간의 차원이 해싱될 고유 토큰의 전체 개수보다 훨씬 크면 해시 충돌의 가능성은 감소합니다.\n",
        "\n",
        "해싱 기법을 사용한 단어 수준의 원-핫 인코딩(간단한 예):"
      ],
      "metadata": {
        "id": "GamG2G3ilenU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# 단어를 크기가 1,000인 벡터로 저장합니다.\n",
        "# 1,000개(또는 그이상)의 단어가 있다면 해싱 충돌이 늘어나고 인코딩의 정확도가 감소될 것입니다\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        # 단어를 해싱하여 0과 1,000 사이의 랜덤한 정수 인덱스로 변환합니다.\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        results[i, j, index] = 1."
      ],
      "metadata": {
        "id": "84iUvV6nl_N0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YX3l6w7mAMh",
        "outputId": "30b44aa8-b03c-4980-dfec-823546ee9014"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gJ6O3B2emCBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}